{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPUoA71ziUD/92/YwwhOrDp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dharnashukla94/Experiments_with_Neon_Framework/blob/master/Exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilt328r62tD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip install nervananeon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML-0coiz096f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from numpy import *\n",
        "from neon.data import MNIST\n",
        "from neon.backends import gen_backend\n",
        "from neon.initializers import Gaussian\n",
        "from neon.layers import Affine\n",
        "from neon.transforms import Rectlin, Softmax\n",
        "from neon.models import Model\n",
        "from neon.layers import GeneralizedCost\n",
        "from neon.transforms import CrossEntropyMulti\n",
        "from neon.optimizers import GradientDescentMomentum\n",
        "from neon.callbacks.callbacks import Callbacks\n",
        "from neon.transforms import Misclassification\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-BL7wfS2G6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting backend\n",
        "\n",
        "batch_size = 56\n",
        "\n",
        "be = gen_backend(batch_size=batch_size, backend='cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYV3vFSe0kJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# downloading Mnist Dataset\n",
        "\n",
        "mnist = MNIST(path='data/') # Mention the path to store Mnist dataset\n",
        "train_set = mnist.train_iter\n",
        "valid_set = mnist.valid_iter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WlVyRwU0umD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing the weights\n",
        "\n",
        "init_norm = Gaussian(loc=0.0, scale=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koqPAII824pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating simple architecture with Feed Forward Network\n",
        "\n",
        "layers = []\n",
        "layers.append(Affine(nout=10, init=init_norm, activation=Rectlin()))\n",
        "layers.append(Affine(nout=10, init=init_norm,\n",
        "                     activation=Softmax()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6jamao130d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initilazing the model\n",
        "\n",
        "mlp = Model(layers=layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kVc3Xrw3_PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declaring cost function\n",
        "\n",
        "cost = GeneralizedCost(costfunc=CrossEntropyMulti())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHdDeJLn43t8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "124a5c3d-2850-40be-eed2-914f629dd9a8"
      },
      "source": [
        "# Optimizers\n",
        "optimizer = GradientDescentMomentum(0.001, momentum_coef=0.9)\n",
        "callbacks = Callbacks(mlp, eval_set=valid_set)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/neon/callbacks/callbacks.py:97: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  self.callback_data = h5py.File(self.name, driver='core', backing_store=False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr7N62MP5EWO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "abee93eb-dd82-4cf0-dffc-3ec3ff92f74f"
      },
      "source": [
        "# GRADED FUNCTION\n",
        "mlp.fit(train_set, optimizer=optimizer, num_epochs=200, cost=cost, callbacks=callbacks)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\rEpoch 0   [Train |                    |   38/1072 batches, 2.30 cost, 0.10s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/neon/backends/nervanacpu.py:680: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  array_output[numpy_ind.tolist()] = 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0   [Train |████████████████████| 1072/1072 batches, 1.55 cost, 2.18s]\n",
            "Epoch 1   [Train |████████████████████| 1071/1071 batches, 0.53 cost, 2.52s]\n",
            "Epoch 2   [Train |████████████████████| 1072/1072 batches, 0.41 cost, 2.58s]\n",
            "Epoch 3   [Train |████████████████████| 1071/1071 batches, 0.33 cost, 2.67s]\n",
            "Epoch 4   [Train |████████████████████| 1072/1072 batches, 0.32 cost, 2.53s]\n",
            "Epoch 5   [Train |████████████████████| 1071/1071 batches, 0.29 cost, 2.45s]\n",
            "Epoch 6   [Train |████████████████████| 1071/1071 batches, 0.29 cost, 2.57s]\n",
            "Epoch 7   [Train |████████████████████| 1071/1071 batches, 0.28 cost, 2.49s]\n",
            "Epoch 8   [Train |████████████████████| 1071/1071 batches, 0.28 cost, 2.53s]\n",
            "Epoch 9   [Train |████████████████████| 1071/1071 batches, 0.27 cost, 2.54s]\n",
            "Epoch 10  [Train |████████████████████| 1071/1071 batches, 0.27 cost, 2.56s]\n",
            "Epoch 11  [Train |████████████████████| 1071/1071 batches, 0.27 cost, 2.50s]\n",
            "Epoch 12  [Train |████████████████████| 1071/1071 batches, 0.26 cost, 2.41s]\n",
            "Epoch 13  [Train |████████████████████| 1071/1071 batches, 0.26 cost, 2.44s]\n",
            "Epoch 14  [Train |████████████████████| 1071/1071 batches, 0.25 cost, 2.53s]\n",
            "Epoch 15  [Train |████████████████████| 1071/1071 batches, 0.25 cost, 2.44s]\n",
            "Epoch 16  [Train |████████████████████| 1071/1071 batches, 0.25 cost, 2.39s]\n",
            "Epoch 17  [Train |████████████████████| 1071/1071 batches, 0.25 cost, 2.40s]\n",
            "Epoch 18  [Train |████████████████████| 1071/1071 batches, 0.24 cost, 2.56s]\n",
            "Epoch 19  [Train |████████████████████| 1071/1071 batches, 0.24 cost, 2.47s]\n",
            "Epoch 20  [Train |████████████████████| 1071/1071 batches, 0.24 cost, 2.36s]\n",
            "Epoch 21  [Train |████████████████████| 1071/1071 batches, 0.23 cost, 2.56s]\n",
            "Epoch 22  [Train |████████████████████| 1071/1071 batches, 0.23 cost, 2.46s]\n",
            "Epoch 23  [Train |████████████████████| 1071/1071 batches, 0.23 cost, 2.56s]\n",
            "Epoch 24  [Train |████████████████████| 1071/1071 batches, 0.23 cost, 2.41s]\n",
            "Epoch 25  [Train |████████████████████| 1071/1071 batches, 0.23 cost, 2.47s]\n",
            "Epoch 26  [Train |████████████████████| 1071/1071 batches, 0.23 cost, 2.49s]\n",
            "Epoch 27  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.40s]\n",
            "Epoch 28  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.49s]\n",
            "Epoch 29  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.36s]\n",
            "Epoch 30  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.37s]\n",
            "Epoch 31  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.60s]\n",
            "Epoch 32  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.41s]\n",
            "Epoch 33  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.53s]\n",
            "Epoch 34  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.48s]\n",
            "Epoch 35  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.46s]\n",
            "Epoch 36  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.56s]\n",
            "Epoch 37  [Train |████████████████████| 1071/1071 batches, 0.22 cost, 2.40s]\n",
            "Epoch 38  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.64s]\n",
            "Epoch 39  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.55s]\n",
            "Epoch 40  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.44s]\n",
            "Epoch 41  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.38s]\n",
            "Epoch 42  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.40s]\n",
            "Epoch 43  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.40s]\n",
            "Epoch 44  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.37s]\n",
            "Epoch 45  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.46s]\n",
            "Epoch 46  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.35s]\n",
            "Epoch 47  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.52s]\n",
            "Epoch 48  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.40s]\n",
            "Epoch 49  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.41s]\n",
            "Epoch 50  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.45s]\n",
            "Epoch 51  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.59s]\n",
            "Epoch 52  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.45s]\n",
            "Epoch 53  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.46s]\n",
            "Epoch 54  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.46s]\n",
            "Epoch 55  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.51s]\n",
            "Epoch 56  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.56s]\n",
            "Epoch 57  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.47s]\n",
            "Epoch 58  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.41s]\n",
            "Epoch 59  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.49s]\n",
            "Epoch 60  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.42s]\n",
            "Epoch 61  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.42s]\n",
            "Epoch 62  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.42s]\n",
            "Epoch 63  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.45s]\n",
            "Epoch 64  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.42s]\n",
            "Epoch 65  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.44s]\n",
            "Epoch 66  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.46s]\n",
            "Epoch 67  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.51s]\n",
            "Epoch 68  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.59s]\n",
            "Epoch 69  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.51s]\n",
            "Epoch 70  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.51s]\n",
            "Epoch 71  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.45s]\n",
            "Epoch 72  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.59s]\n",
            "Epoch 73  [Train |████████████████████| 1071/1071 batches, 0.21 cost, 2.38s]\n",
            "Epoch 74  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.51s]\n",
            "Epoch 75  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.38s]\n",
            "Epoch 76  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.52s]\n",
            "Epoch 77  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.46s]\n",
            "Epoch 78  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.50s]\n",
            "Epoch 79  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.41s]\n",
            "Epoch 80  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.44s]\n",
            "Epoch 81  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.53s]\n",
            "Epoch 82  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.38s]\n",
            "Epoch 83  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.40s]\n",
            "Epoch 84  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.36s]\n",
            "Epoch 85  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.52s]\n",
            "Epoch 86  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.44s]\n",
            "Epoch 87  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.47s]\n",
            "Epoch 88  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.50s]\n",
            "Epoch 89  [Train |████████████████████| 1071/1071 batches, 0.20 cost, 2.45s]\n",
            "Epoch 90  [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.36s]\n",
            "Epoch 91  [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.35s]\n",
            "Epoch 92  [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.39s]\n",
            "Epoch 93  [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.38s]\n",
            "Epoch 94  [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.45s]\n",
            "Epoch 95  [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.35s]\n",
            "Epoch 96  [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.38s]\n",
            "Epoch 97  [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.49s]\n",
            "Epoch 98  [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.48s]\n",
            "Epoch 99  [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.50s]\n",
            "Epoch 100 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.59s]\n",
            "Epoch 101 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.44s]\n",
            "Epoch 102 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.37s]\n",
            "Epoch 103 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.37s]\n",
            "Epoch 104 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.58s]\n",
            "Epoch 105 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.46s]\n",
            "Epoch 106 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.45s]\n",
            "Epoch 107 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.37s]\n",
            "Epoch 108 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.35s]\n",
            "Epoch 109 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.47s]\n",
            "Epoch 110 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.43s]\n",
            "Epoch 111 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.41s]\n",
            "Epoch 112 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.41s]\n",
            "Epoch 113 [Train |████████████████████| 1071/1071 batches, 0.19 cost, 2.39s]\n",
            "Epoch 114 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.35s]\n",
            "Epoch 115 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.49s]\n",
            "Epoch 116 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.42s]\n",
            "Epoch 117 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.35s]\n",
            "Epoch 118 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.54s]\n",
            "Epoch 119 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.43s]\n",
            "Epoch 120 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.55s]\n",
            "Epoch 121 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.46s]\n",
            "Epoch 122 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.46s]\n",
            "Epoch 123 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.40s]\n",
            "Epoch 124 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.38s]\n",
            "Epoch 125 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.35s]\n",
            "Epoch 126 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.49s]\n",
            "Epoch 127 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.36s]\n",
            "Epoch 128 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.35s]\n",
            "Epoch 129 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.50s]\n",
            "Epoch 130 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.38s]\n",
            "Epoch 131 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.47s]\n",
            "Epoch 132 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.47s]\n",
            "Epoch 133 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.39s]\n",
            "Epoch 134 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.41s]\n",
            "Epoch 135 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.44s]\n",
            "Epoch 136 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.46s]\n",
            "Epoch 137 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.47s]\n",
            "Epoch 138 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.36s]\n",
            "Epoch 139 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.39s]\n",
            "Epoch 140 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.50s]\n",
            "Epoch 141 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.54s]\n",
            "Epoch 142 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.51s]\n",
            "Epoch 143 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.34s]\n",
            "Epoch 144 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.43s]\n",
            "Epoch 145 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.47s]\n",
            "Epoch 146 [Train |████████████████████| 1071/1071 batches, 0.18 cost, 2.49s]\n",
            "Epoch 147 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.52s]\n",
            "Epoch 148 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.47s]\n",
            "Epoch 149 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.42s]\n",
            "Epoch 150 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.41s]\n",
            "Epoch 151 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.39s]\n",
            "Epoch 152 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.37s]\n",
            "Epoch 153 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.62s]\n",
            "Epoch 154 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.49s]\n",
            "Epoch 155 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.48s]\n",
            "Epoch 156 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.40s]\n",
            "Epoch 157 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.53s]\n",
            "Epoch 158 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.43s]\n",
            "Epoch 159 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.43s]\n",
            "Epoch 160 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.41s]\n",
            "Epoch 161 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.44s]\n",
            "Epoch 162 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.41s]\n",
            "Epoch 163 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.45s]\n",
            "Epoch 164 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.48s]\n",
            "Epoch 165 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.56s]\n",
            "Epoch 166 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.49s]\n",
            "Epoch 167 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.51s]\n",
            "Epoch 168 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.40s]\n",
            "Epoch 169 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.54s]\n",
            "Epoch 170 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.51s]\n",
            "Epoch 171 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.42s]\n",
            "Epoch 172 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.49s]\n",
            "Epoch 173 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.43s]\n",
            "Epoch 174 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.49s]\n",
            "Epoch 175 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.42s]\n",
            "Epoch 176 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.38s]\n",
            "Epoch 177 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.37s]\n",
            "Epoch 178 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.61s]\n",
            "Epoch 179 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.41s]\n",
            "Epoch 180 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.50s]\n",
            "Epoch 181 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.39s]\n",
            "Epoch 182 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.43s]\n",
            "Epoch 183 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.40s]\n",
            "Epoch 184 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.67s]\n",
            "Epoch 185 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.51s]\n",
            "Epoch 186 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.53s]\n",
            "Epoch 187 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.42s]\n",
            "Epoch 188 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.52s]\n",
            "Epoch 189 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.52s]\n",
            "Epoch 190 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.52s]\n",
            "Epoch 191 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.35s]\n",
            "Epoch 192 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.35s]\n",
            "Epoch 193 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.37s]\n",
            "Epoch 194 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.61s]\n",
            "Epoch 195 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.57s]\n",
            "Epoch 196 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.46s]\n",
            "Epoch 197 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.43s]\n",
            "Epoch 198 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.60s]\n",
            "Epoch 199 [Train |████████████████████| 1071/1071 batches, 0.17 cost, 2.45s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OOSBM0l5Zvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f07e878c-476c-4db2-d0ce-215836e6536b"
      },
      "source": [
        "results = mlp.get_outputs(valid_set)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/neon/backends/nervanacpu.py:680: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  array_output[numpy_ind.tolist()] = 1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpCemyx45d1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7dc2bb49-5504-40f3-d0fa-ddf5ef0047da"
      },
      "source": [
        "# evaluate the model on test_set using the misclassification metric\n",
        "error = mlp.eval(valid_set, metric=Misclassification())*100\n",
        "print('Misclassification error = %.1f%%' % error)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Misclassification error = 6.0%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/neon/backends/nervanacpu.py:680: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  array_output[numpy_ind.tolist()] = 1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN0-SSunKR4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}