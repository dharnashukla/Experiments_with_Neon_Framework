{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNXOTO2SMElbuagpPUBBpNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dharnashukla94/Experiments_with_Neon_Framework/blob/master/Exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilt328r62tD1",
        "colab_type": "code",
        "outputId": "466891e2-d052-4cc8-d2a7-6f6992e2b00e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#pip install nervananeon"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nervananeon\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/71/4e8f2c1ff5318d21952a80b83d8eeb6bf48c7c3a7f41ceac97d6e8b8e537/nervananeon-2.6.0-py3-none-any.whl (78.5MB)\n",
            "\u001b[K     |████████████████████████████████| 78.5MB 42kB/s \n",
            "\u001b[?25hCollecting configargparse\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/79/3045743bb26ca2e44a1d317c37395462bfed82dbbd38e69a3280b63696ce/ConfigArgParse-1.2.3.tar.gz (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from nervananeon) (1.14.0)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.6/dist-packages (from nervananeon) (1.8.5)\n",
            "Collecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/6e/36419ec1bd2208e157dff7fc3e565b185394c0dc4901e9e2f983cb1d4b7f/pylint-2.5.2-py3-none-any.whl (324kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 54.8MB/s \n",
            "\u001b[?25hCollecting flake8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/35/dcf9a3393305bfc61854b764b5aeb79a72493e77991eead133c189d7868e/flake8-3.8.2-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.7MB/s \n",
            "\u001b[?25hCollecting appdirs\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from nervananeon) (2.10.0)\n",
            "Collecting pep8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/3f/669429ce58de2c22d8d2c542752e137ec4b9885fff398d3eceb1a7f5acb4/pep8-1.7.1-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.5MB/s \n",
            "\u001b[?25hCollecting pytest-mock\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/e1/f8a889f917b295c04bfc0ab7135405d59eff9ed9e8519fd74dd97afc1566/pytest_mock-3.1.1-py3-none-any.whl\n",
            "Collecting posix-ipc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/3e/54217da71aa26b488295d878df4d3132093253b4ae5798ac66fcb6921ef0/posix_ipc-1.0.4.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nervananeon) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from nervananeon) (0.16.0)\n",
            "Collecting pandoc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/b1/d2d4b30ee81ea5cb7aee5ba3591752a637fdc49d0a42fa9683874b60b9fb/pandoc-1.0.2.tar.gz (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 42.7MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/60/63f28a5401da733043abe7053e7d9591491b4784c4f87c339bf51215aa0a/py-cpuinfo-5.0.0.tar.gz (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.9MB/s \n",
            "\u001b[?25hCollecting pytest-cov\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/4d/56896f913ea61f0ba504c912bdd748f11e45ef14d5d40975bc8322dccfa0/pytest_cov-2.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nervananeon) (4.41.1)\n",
            "Collecting pypandoc\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/b7/5050dc1769c8a93d3ec7c4bd55be161991c94b8b235f88bf7c764449e708/pypandoc-1.5.tar.gz\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nervananeon) (3.0.12)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from nervananeon) (3.6.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from nervananeon) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from nervananeon) (3.13)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->nervananeon) (2.20)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (0.15.2)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (2.8.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (20.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (2.1.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (2.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (47.1.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (1.12.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->nervananeon) (2.11.2)\n",
            "Collecting isort<5,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting astroid<=2.5,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/c9/e9c2642dfb169590fb8bdb395f9329da042ee559c2ae7c1e612a3e5f40b4/astroid-2.4.1-py3-none-any.whl (214kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 59.7MB/s \n",
            "\u001b[?25hCollecting toml>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/e1/1b40b80f2e1663a6b9f497123c11d7d988c0919abbf3c3f2688e448c5363/toml-0.10.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from flake8->nervananeon) (1.6.0)\n",
            "Collecting pycodestyle<2.7.0,>=2.6.0a1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hCollecting pyflakes<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/5b/fd01b0c696f2f9a6d2c839883b642493b431f28fa32b29abc465ef675473/pyflakes-2.2.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.3MB/s \n",
            "\u001b[?25hCollecting ply\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/58/35da89ee790598a0700ea49b2a66594140f44dec458c07e8e3d4979137fc/ply-3.11-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hCollecting coverage>=4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/3e/fc18ecef69f174c13493576f46966053c1da07fd8721962530dc1a10b1ca/coverage-5.1-cp36-cp36m-manylinux1_x86_64.whl (227kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pip>=8.1.0 in /usr/local/lib/python3.6/dist-packages (from pypandoc->nervananeon) (19.3.1)\n",
            "Requirement already satisfied: wheel>=0.25.0 in /usr/local/lib/python3.6/dist-packages (from pypandoc->nervananeon) (0.34.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->nervananeon) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->nervananeon) (8.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->nervananeon) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->nervananeon) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->nervananeon) (19.3.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel!=2.0,>=1.3->sphinx->nervananeon) (2018.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->sphinx->nervananeon) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->nervananeon) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->nervananeon) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->nervananeon) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->nervananeon) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx->nervananeon) (1.1.1)\n",
            "Collecting lazy-object-proxy==1.4.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt~=1.11 in /usr/local/lib/python3.6/dist-packages (from astroid<=2.5,>=2.4.0->pylint->nervananeon) (1.12.1)\n",
            "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->flake8->nervananeon) (3.1.0)\n",
            "Building wheels for collected packages: configargparse, posix-ipc, pandoc, py-cpuinfo, pypandoc\n",
            "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configargparse: filename=ConfigArgParse-1.2.3-cp36-none-any.whl size=19328 sha256=4a57122af8e663c2e79684ef3b3b1b57dfe9da894ed1c76800acdaf5d6f94a12\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/d6/53/034032da9498bda2385cd50a51a289e88090b5da2d592b1fdf\n",
            "  Building wheel for posix-ipc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for posix-ipc: filename=posix_ipc-1.0.4-cp36-cp36m-linux_x86_64.whl size=45099 sha256=b710956ff8f61c6317314cef53b4251a8361e1f5cc5b740806fc4c844531c2ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/5c/17/d0e2d421abaf3b4097bf9db11108380d734195c6d15c24269d\n",
            "  Building wheel for pandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandoc: filename=pandoc-1.0.2-cp36-none-any.whl size=19993 sha256=6c3d35db69a6ba92e663e92371e7907ed946c21d48957bc8aee5a71e12bac181\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/e8/71/bc3242b3e8f119c62eebdb0dee519fd40ac293e4835839db7c\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-5.0.0-cp36-none-any.whl size=18684 sha256=7a11ba1a6ced3e7b26d5aa37e49352805f1f553b743be593812a65185c7a4975\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/7e/a9/b982d0fea22b7e4ae5619de949570cde5ad55420cec16e86a5\n",
            "  Building wheel for pypandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypandoc: filename=pypandoc-1.5-cp36-none-any.whl size=17037 sha256=691440489a24b13ea26bc41e75b895c06186085fb3d119fd846e98855f9dea62\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/7d/d6/2f9af55e800d37e42e546106bcbd36a86e24e725e303d17e04\n",
            "Successfully built configargparse posix-ipc pandoc py-cpuinfo pypandoc\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement coverage==3.7.1, but you'll have coverage 5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: coveralls 0.5 has requirement coverage<3.999,>=3.6, but you'll have coverage 5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: configargparse, isort, mccabe, lazy-object-proxy, typed-ast, astroid, toml, pylint, pycodestyle, pyflakes, flake8, appdirs, pep8, pytest-mock, posix-ipc, ply, pandoc, py-cpuinfo, coverage, pytest-cov, pypandoc, nervananeon\n",
            "  Found existing installation: coverage 3.7.1\n",
            "    Uninstalling coverage-3.7.1:\n",
            "      Successfully uninstalled coverage-3.7.1\n",
            "Successfully installed appdirs-1.4.4 astroid-2.4.1 configargparse-1.2.3 coverage-5.1 flake8-3.8.2 isort-4.3.21 lazy-object-proxy-1.4.3 mccabe-0.6.1 nervananeon-2.6.0 pandoc-1.0.2 pep8-1.7.1 ply-3.11 posix-ipc-1.0.4 py-cpuinfo-5.0.0 pycodestyle-2.6.0 pyflakes-2.2.0 pylint-2.5.2 pypandoc-1.5 pytest-cov-2.9.0 pytest-mock-3.1.1 toml-0.10.1 typed-ast-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML-0coiz096f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from numpy import *\n",
        "from neon.data import MNIST\n",
        "from neon.backends import gen_backend\n",
        "from neon.initializers import Gaussian\n",
        "from neon.layers import Affine\n",
        "from neon.transforms import Rectlin, Softmax\n",
        "from neon.models import Model\n",
        "from neon.layers import GeneralizedCost\n",
        "from neon.transforms import CrossEntropyMulti\n",
        "from neon.optimizers import GradientDescentMomentum\n",
        "from neon.callbacks.callbacks import Callbacks\n",
        "from neon.transforms import Misclassification\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import data, color\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-BL7wfS2G6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting backend\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "be = gen_backend(batch_size=batch_size, backend='cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYV3vFSe0kJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# downloading Mnist Dataset\n",
        "\n",
        "mnist = MNIST(path='data/') # Mention the path to store Mnist dataset\n",
        "train_set = mnist.train_iter\n",
        "valid_set = mnist.valid_iter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WlVyRwU0umD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing the weights\n",
        "\n",
        "init_norm = Gaussian(loc=0.0, scale=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koqPAII824pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating simple architecture with Feed Forward Network\n",
        "\n",
        "layers = []\n",
        "layers.append(Affine(nout=10, init=init_norm, activation=Rectlin()))\n",
        "layers.append(Affine(nout=10, init=init_norm,\n",
        "                     activation=Softmax()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6jamao130d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initilazing the model\n",
        "\n",
        "mlp = Model(layers=layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kVc3Xrw3_PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declaring cost function\n",
        "\n",
        "cost = GeneralizedCost(costfunc=CrossEntropyMulti())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHdDeJLn43t8",
        "colab_type": "code",
        "outputId": "dede2d62-f11e-4f98-9a3f-77424f3fcd96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Optimizers\n",
        "optimizer = GradientDescentMomentum(0.001, momentum_coef=0.9)\n",
        "callbacks = Callbacks(mlp, eval_set=valid_set)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/neon/callbacks/callbacks.py:97: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  self.callback_data = h5py.File(self.name, driver='core', backing_store=False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr7N62MP5EWO",
        "colab_type": "code",
        "outputId": "bc582f5e-b1b6-4a5b-b1ab-61da0cd37a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# GRADED FUNCTION\n",
        "mlp.fit(train_set, optimizer=optimizer, num_epochs=500, cost=cost, callbacks=callbacks)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\rEpoch 100 [Train |█                   |   31/468  batches, 0.37 cost, 0.10s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/neon/backends/nervanacpu.py:680: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  array_output[numpy_ind.tolist()] = 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 100 [Train |████████████████████|  468/468  batches, 0.28 cost, 1.30s]\n",
            "Epoch 101 [Train |████████████████████|  468/468  batches, 0.27 cost, 1.24s]\n",
            "Epoch 102 [Train |████████████████████|  468/468  batches, 0.26 cost, 1.22s]\n",
            "Epoch 103 [Train |████████████████████|  468/468  batches, 0.25 cost, 1.26s]\n",
            "Epoch 104 [Train |████████████████████|  468/468  batches, 0.25 cost, 1.27s]\n",
            "Epoch 105 [Train |████████████████████|  468/468  batches, 0.25 cost, 1.23s]\n",
            "Epoch 106 [Train |████████████████████|  468/468  batches, 0.24 cost, 1.26s]\n",
            "Epoch 107 [Train |████████████████████|  468/468  batches, 0.24 cost, 1.24s]\n",
            "Epoch 108 [Train |████████████████████|  468/468  batches, 0.24 cost, 1.27s]\n",
            "Epoch 109 [Train |████████████████████|  468/468  batches, 0.23 cost, 1.27s]\n",
            "Epoch 110 [Train |████████████████████|  468/468  batches, 0.23 cost, 1.29s]\n",
            "Epoch 111 [Train |████████████████████|  468/468  batches, 0.23 cost, 1.31s]\n",
            "Epoch 112 [Train |████████████████████|  468/468  batches, 0.23 cost, 1.30s]\n",
            "Epoch 113 [Train |████████████████████|  468/468  batches, 0.23 cost, 1.30s]\n",
            "Epoch 114 [Train |████████████████████|  468/468  batches, 0.22 cost, 1.31s]\n",
            "Epoch 115 [Train |████████████████████|  468/468  batches, 0.22 cost, 1.27s]\n",
            "Epoch 116 [Train |████████████████████|  468/468  batches, 0.22 cost, 1.23s]\n",
            "Epoch 117 [Train |████████████████████|  468/468  batches, 0.22 cost, 1.23s]\n",
            "Epoch 118 [Train |████████████████████|  468/468  batches, 0.22 cost, 1.24s]\n",
            "Epoch 119 [Train |████████████████████|  468/468  batches, 0.22 cost, 1.24s]\n",
            "Epoch 120 [Train |████████████████████|  468/468  batches, 0.22 cost, 1.23s]\n",
            "Epoch 121 [Train |████████████████████|  468/468  batches, 0.21 cost, 1.26s]\n",
            "Epoch 122 [Train |████████████████████|  468/468  batches, 0.21 cost, 1.25s]\n",
            "Epoch 123 [Train |████████████████████|  468/468  batches, 0.21 cost, 1.29s]\n",
            "Epoch 124 [Train |████████████████████|  468/468  batches, 0.21 cost, 1.30s]\n",
            "Epoch 125 [Train |████████████████████|  468/468  batches, 0.21 cost, 1.23s]\n",
            "Epoch 126 [Train |████████████████████|  468/468  batches, 0.21 cost, 1.23s]\n",
            "Epoch 127 [Train |████████████████████|  468/468  batches, 0.21 cost, 1.26s]\n",
            "Epoch 128 [Train |████████████████████|  468/468  batches, 0.21 cost, 1.29s]\n",
            "Epoch 129 [Train |████████████████████|  468/468  batches, 0.21 cost, 1.26s]\n",
            "Epoch 130 [Train |████████████████████|  468/468  batches, 0.21 cost, 1.27s]\n",
            "Epoch 131 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.23s]\n",
            "Epoch 132 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.26s]\n",
            "Epoch 133 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.26s]\n",
            "Epoch 134 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.26s]\n",
            "Epoch 135 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.31s]\n",
            "Epoch 136 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.25s]\n",
            "Epoch 137 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.31s]\n",
            "Epoch 138 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.29s]\n",
            "Epoch 139 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.24s]\n",
            "Epoch 140 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.25s]\n",
            "Epoch 141 [Train |████████████████████|  468/468  batches, 0.20 cost, 1.24s]\n",
            "Epoch 142 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.23s]\n",
            "Epoch 143 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.26s]\n",
            "Epoch 144 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.27s]\n",
            "Epoch 145 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.26s]\n",
            "Epoch 146 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.24s]\n",
            "Epoch 147 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.31s]\n",
            "Epoch 148 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.29s]\n",
            "Epoch 149 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.22s]\n",
            "Epoch 150 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.25s]\n",
            "Epoch 151 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.23s]\n",
            "Epoch 152 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.23s]\n",
            "Epoch 153 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.30s]\n",
            "Epoch 154 [Train |████████████████████|  468/468  batches, 0.19 cost, 1.20s]\n",
            "Epoch 155 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.23s]\n",
            "Epoch 156 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.30s]\n",
            "Epoch 157 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.24s]\n",
            "Epoch 158 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.33s]\n",
            "Epoch 159 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.30s]\n",
            "Epoch 160 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.23s]\n",
            "Epoch 161 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.30s]\n",
            "Epoch 162 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.25s]\n",
            "Epoch 163 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.25s]\n",
            "Epoch 164 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.33s]\n",
            "Epoch 165 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.30s]\n",
            "Epoch 166 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.23s]\n",
            "Epoch 167 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.26s]\n",
            "Epoch 168 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.26s]\n",
            "Epoch 169 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.30s]\n",
            "Epoch 170 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.26s]\n",
            "Epoch 171 [Train |████████████████████|  468/468  batches, 0.18 cost, 1.28s]\n",
            "Epoch 172 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.27s]\n",
            "Epoch 173 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.24s]\n",
            "Epoch 174 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.29s]\n",
            "Epoch 175 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.31s]\n",
            "Epoch 176 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.36s]\n",
            "Epoch 177 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.37s]\n",
            "Epoch 178 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.28s]\n",
            "Epoch 179 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.28s]\n",
            "Epoch 180 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.33s]\n",
            "Epoch 181 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.24s]\n",
            "Epoch 182 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.24s]\n",
            "Epoch 183 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.25s]\n",
            "Epoch 184 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.25s]\n",
            "Epoch 185 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.27s]\n",
            "Epoch 186 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.26s]\n",
            "Epoch 187 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.28s]\n",
            "Epoch 188 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.28s]\n",
            "Epoch 189 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.23s]\n",
            "Epoch 190 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.24s]\n",
            "Epoch 191 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.33s]\n",
            "Epoch 192 [Train |████████████████████|  468/468  batches, 0.17 cost, 1.24s]\n",
            "Epoch 193 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.28s]\n",
            "Epoch 194 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.26s]\n",
            "Epoch 195 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.24s]\n",
            "Epoch 196 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.25s]\n",
            "Epoch 197 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.30s]\n",
            "Epoch 198 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.25s]\n",
            "Epoch 199 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.32s]\n",
            "Epoch 200 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.23s]\n",
            "Epoch 201 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.27s]\n",
            "Epoch 202 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.25s]\n",
            "Epoch 203 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.27s]\n",
            "Epoch 204 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.24s]\n",
            "Epoch 205 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.24s]\n",
            "Epoch 206 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.22s]\n",
            "Epoch 207 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.23s]\n",
            "Epoch 208 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.23s]\n",
            "Epoch 209 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.31s]\n",
            "Epoch 210 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.31s]\n",
            "Epoch 211 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.27s]\n",
            "Epoch 212 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.25s]\n",
            "Epoch 213 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.22s]\n",
            "Epoch 214 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.25s]\n",
            "Epoch 215 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.24s]\n",
            "Epoch 216 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.26s]\n",
            "Epoch 217 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.26s]\n",
            "Epoch 218 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.26s]\n",
            "Epoch 219 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.29s]\n",
            "Epoch 220 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.26s]\n",
            "Epoch 221 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.23s]\n",
            "Epoch 222 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.24s]\n",
            "Epoch 223 [Train |████████████████████|  468/468  batches, 0.16 cost, 1.23s]\n",
            "Epoch 224 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.29s]\n",
            "Epoch 225 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.27s]\n",
            "Epoch 226 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.28s]\n",
            "Epoch 227 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.23s]\n",
            "Epoch 228 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.27s]\n",
            "Epoch 229 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.28s]\n",
            "Epoch 230 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.29s]\n",
            "Epoch 231 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.25s]\n",
            "Epoch 232 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.27s]\n",
            "Epoch 233 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.34s]\n",
            "Epoch 234 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.25s]\n",
            "Epoch 235 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.25s]\n",
            "Epoch 236 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.31s]\n",
            "Epoch 237 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.25s]\n",
            "Epoch 238 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.24s]\n",
            "Epoch 239 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.26s]\n",
            "Epoch 240 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.35s]\n",
            "Epoch 241 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.28s]\n",
            "Epoch 242 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.25s]\n",
            "Epoch 243 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.27s]\n",
            "Epoch 244 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.31s]\n",
            "Epoch 245 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.23s]\n",
            "Epoch 246 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.23s]\n",
            "Epoch 247 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.23s]\n",
            "Epoch 248 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.25s]\n",
            "Epoch 249 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.29s]\n",
            "Epoch 250 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.24s]\n",
            "Epoch 251 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.21s]\n",
            "Epoch 252 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.23s]\n",
            "Epoch 253 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.22s]\n",
            "Epoch 254 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.30s]\n",
            "Epoch 255 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.23s]\n",
            "Epoch 256 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.25s]\n",
            "Epoch 257 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.39s]\n",
            "Epoch 258 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.31s]\n",
            "Epoch 259 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.24s]\n",
            "Epoch 260 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.27s]\n",
            "Epoch 261 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.24s]\n",
            "Epoch 262 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.26s]\n",
            "Epoch 263 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.25s]\n",
            "Epoch 264 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.28s]\n",
            "Epoch 265 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.29s]\n",
            "Epoch 266 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.27s]\n",
            "Epoch 267 [Train |████████████████████|  468/468  batches, 0.15 cost, 1.23s]\n",
            "Epoch 268 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.25s]\n",
            "Epoch 269 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.25s]\n",
            "Epoch 270 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.22s]\n",
            "Epoch 271 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.23s]\n",
            "Epoch 272 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.24s]\n",
            "Epoch 273 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.24s]\n",
            "Epoch 274 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.25s]\n",
            "Epoch 275 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.24s]\n",
            "Epoch 276 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.25s]\n",
            "Epoch 277 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.28s]\n",
            "Epoch 278 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.37s]\n",
            "Epoch 279 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.24s]\n",
            "Epoch 280 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.26s]\n",
            "Epoch 281 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.27s]\n",
            "Epoch 282 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.26s]\n",
            "Epoch 283 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.26s]\n",
            "Epoch 284 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.34s]\n",
            "Epoch 285 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.30s]\n",
            "Epoch 286 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.26s]\n",
            "Epoch 287 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.23s]\n",
            "Epoch 288 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.25s]\n",
            "Epoch 289 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.33s]\n",
            "Epoch 290 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.33s]\n",
            "Epoch 291 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.23s]\n",
            "Epoch 292 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.25s]\n",
            "Epoch 293 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.26s]\n",
            "Epoch 294 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.27s]\n",
            "Epoch 295 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.31s]\n",
            "Epoch 296 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.27s]\n",
            "Epoch 297 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.30s]\n",
            "Epoch 298 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.30s]\n",
            "Epoch 299 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.26s]\n",
            "Epoch 300 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.29s]\n",
            "Epoch 301 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.36s]\n",
            "Epoch 302 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.25s]\n",
            "Epoch 303 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.27s]\n",
            "Epoch 304 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.32s]\n",
            "Epoch 305 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.27s]\n",
            "Epoch 306 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.26s]\n",
            "Epoch 307 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.29s]\n",
            "Epoch 308 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.27s]\n",
            "Epoch 309 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.27s]\n",
            "Epoch 310 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.30s]\n",
            "Epoch 311 [Train |████████████████████|  468/468  batches, 0.14 cost, 1.29s]\n",
            "Epoch 312 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.23s]\n",
            "Epoch 313 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.22s]\n",
            "Epoch 314 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.32s]\n",
            "Epoch 315 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.25s]\n",
            "Epoch 316 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 317 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.23s]\n",
            "Epoch 318 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 319 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.32s]\n",
            "Epoch 320 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 321 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.23s]\n",
            "Epoch 322 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 323 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.29s]\n",
            "Epoch 324 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.34s]\n",
            "Epoch 325 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.30s]\n",
            "Epoch 326 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.25s]\n",
            "Epoch 327 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 328 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.25s]\n",
            "Epoch 329 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 330 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.29s]\n",
            "Epoch 331 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.30s]\n",
            "Epoch 332 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.25s]\n",
            "Epoch 333 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.29s]\n",
            "Epoch 334 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 335 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.28s]\n",
            "Epoch 336 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.28s]\n",
            "Epoch 337 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 338 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 339 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.28s]\n",
            "Epoch 340 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.30s]\n",
            "Epoch 341 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 342 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 343 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.28s]\n",
            "Epoch 344 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.23s]\n",
            "Epoch 345 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.28s]\n",
            "Epoch 346 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.27s]\n",
            "Epoch 347 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.25s]\n",
            "Epoch 348 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.23s]\n",
            "Epoch 349 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.23s]\n",
            "Epoch 350 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.23s]\n",
            "Epoch 351 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.28s]\n",
            "Epoch 352 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.23s]\n",
            "Epoch 353 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.22s]\n",
            "Epoch 354 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.27s]\n",
            "Epoch 355 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 356 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 357 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.33s]\n",
            "Epoch 358 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 359 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.27s]\n",
            "Epoch 360 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.27s]\n",
            "Epoch 361 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.27s]\n",
            "Epoch 362 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.30s]\n",
            "Epoch 363 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.27s]\n",
            "Epoch 364 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.28s]\n",
            "Epoch 365 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 366 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.30s]\n",
            "Epoch 367 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.31s]\n",
            "Epoch 368 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 369 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.22s]\n",
            "Epoch 370 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 371 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.32s]\n",
            "Epoch 372 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.22s]\n",
            "Epoch 373 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 374 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 375 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 376 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.29s]\n",
            "Epoch 377 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 378 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.27s]\n",
            "Epoch 379 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.25s]\n",
            "Epoch 380 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.31s]\n",
            "Epoch 381 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.30s]\n",
            "Epoch 382 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 383 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.27s]\n",
            "Epoch 384 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.34s]\n",
            "Epoch 385 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.25s]\n",
            "Epoch 386 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.29s]\n",
            "Epoch 387 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.28s]\n",
            "Epoch 388 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.30s]\n",
            "Epoch 389 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 390 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.26s]\n",
            "Epoch 391 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.25s]\n",
            "Epoch 392 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.25s]\n",
            "Epoch 393 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.23s]\n",
            "Epoch 394 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.32s]\n",
            "Epoch 395 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.33s]\n",
            "Epoch 396 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.28s]\n",
            "Epoch 397 [Train |████████████████████|  468/468  batches, 0.13 cost, 1.24s]\n",
            "Epoch 398 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.27s]\n",
            "Epoch 399 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 400 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 401 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 402 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.32s]\n",
            "Epoch 403 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 404 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 405 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n",
            "Epoch 406 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.27s]\n",
            "Epoch 407 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 408 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 409 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.27s]\n",
            "Epoch 410 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.32s]\n",
            "Epoch 411 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n",
            "Epoch 412 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 413 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 414 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 415 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 416 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 417 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 418 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.28s]\n",
            "Epoch 419 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 420 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 421 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n",
            "Epoch 422 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n",
            "Epoch 423 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.27s]\n",
            "Epoch 424 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 425 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.27s]\n",
            "Epoch 426 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n",
            "Epoch 427 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.31s]\n",
            "Epoch 428 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 429 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.31s]\n",
            "Epoch 430 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 431 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.33s]\n",
            "Epoch 432 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 433 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 434 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.27s]\n",
            "Epoch 435 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n",
            "Epoch 436 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 437 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 438 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.35s]\n",
            "Epoch 439 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.34s]\n",
            "Epoch 440 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 441 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 442 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.34s]\n",
            "Epoch 443 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 444 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 445 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 446 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 447 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 448 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 449 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 450 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.35s]\n",
            "Epoch 451 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 452 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.30s]\n",
            "Epoch 453 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.27s]\n",
            "Epoch 454 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.37s]\n",
            "Epoch 455 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 456 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.35s]\n",
            "Epoch 457 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.38s]\n",
            "Epoch 458 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.28s]\n",
            "Epoch 459 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 460 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 461 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 462 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 463 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 464 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 465 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.26s]\n",
            "Epoch 466 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n",
            "Epoch 467 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.37s]\n",
            "Epoch 468 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 469 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 470 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.21s]\n",
            "Epoch 471 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 472 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 473 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.23s]\n",
            "Epoch 474 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.34s]\n",
            "Epoch 475 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 476 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.22s]\n",
            "Epoch 477 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 478 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.22s]\n",
            "Epoch 479 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 480 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 481 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n",
            "Epoch 482 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.30s]\n",
            "Epoch 483 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 484 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.27s]\n",
            "Epoch 485 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.27s]\n",
            "Epoch 486 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n",
            "Epoch 487 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.35s]\n",
            "Epoch 488 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n",
            "Epoch 489 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.27s]\n",
            "Epoch 490 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.36s]\n",
            "Epoch 491 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.28s]\n",
            "Epoch 492 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.28s]\n",
            "Epoch 493 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 494 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 495 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.25s]\n",
            "Epoch 496 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 497 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.24s]\n",
            "Epoch 498 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.31s]\n",
            "Epoch 499 [Train |████████████████████|  468/468  batches, 0.12 cost, 1.29s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OOSBM0l5Zvz",
        "colab_type": "code",
        "outputId": "d059de94-0cae-4c58-9fe1-87919529a5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "results = mlp.get_outputs(valid_set)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/neon/backends/nervanacpu.py:680: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  array_output[numpy_ind.tolist()] = 1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpCemyx45d1Q",
        "colab_type": "code",
        "outputId": "7ef164e8-c7f7-4aec-b144-2e64c97fcf6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# evaluate the model on test_set using the misclassification metric\n",
        "error = mlp.eval(valid_set, metric=Misclassification())*100\n",
        "print('Misclassification error = %.1f%%' % error)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Misclassification error = 5.4%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/neon/backends/nervanacpu.py:680: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  array_output[numpy_ind.tolist()] = 1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rnsg8lEJLpJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzipping file for inference\n",
        "\n",
        "with gzip.open('data/mnist.pkl.gz', 'rb') as f:\n",
        "    train_set, _ = pickle.load(f, encoding = 'ISO-8859-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sebkYdb-WnnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgwTDcL0SMbT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d9682a13-c3b9-469c-8e94-600fe4feb939"
      },
      "source": [
        "img = []\n",
        "for i in train_set[0]:\n",
        "    img.append(i)\n",
        "\n",
        "plt.imshow(img[2],cmap='gray')\n",
        "print(img[1].shape)\n",
        "plt.savefig('data/digit2.jpg', cmap='gray')\n",
        "plt.show()    "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq9oiYcbS_1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scale to 28x28 pixels\n",
        "\n",
        "im = Image.open('data/digit2.jpg')\n",
        "im = im.resize((28,28),Image.ANTIALIAS)\n",
        "#im.size\n",
        "digit = np.asarray(im, dtype=np.float32)[:, :, 0]\n",
        "\n",
        "# reshape to a single feature vector\n",
        "digit = digit.reshape(784, 1)\n",
        "\n",
        "# store digit into a GPU tensor\n",
        "x_new = be.zeros((28*28, batch_size), dtype=np.float32)\n",
        "x_new[:, 0] = digit"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}